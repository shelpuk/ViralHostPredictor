{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data leakage reproduction (response to the paper Predicting reservoir hosts and arthropod vectors from evolutionary signatures in RNA virus genomes).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM81DQrwzM0MUm8cZ7/8eNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shelpuk/ViralHostPredictor/blob/master/Data_leakage_reproduction_(response_to_the_paper_Predicting_reservoir_hosts_and_arthropod_vectors_from_evolutionary_signatures_in_RNA_virus_genomes).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI1-jxkVq0YC",
        "colab_type": "text"
      },
      "source": [
        "This code illustrates infinitely powerful data leakage introduced by the inference mechanism suggested in chapter \"Study-wide accuracies\" of the Supplementary Materials for \"Predicting reservoir hosts and arthropod vectors from evolutionary signatures in RNA virus genomes\" by Simon A. Babayan,\n",
        "Richard J. Orton, Daniel G. Streicker, published 2 November 2018, Science 362, 577 (2018), DOI: 10.1126/science.aap9072\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l54yEAGQqqPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFdSw34rciP",
        "colab_type": "text"
      },
      "source": [
        "Class `RandomClassifier` implements random classifier. For every data point, it assigns a random class label. The classifier assigns this label consistently. I.e. if you request prediction for the same data point several times, the predicted class is going to be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6TTfmmfrZH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomClassifier(object):\n",
        "    def __init__(self, num_classes):\n",
        "        \"\"\"\n",
        "        This class implements random classifier. For every data point, it assigns a random class label.\n",
        "        The classifier assigns this label consistently. I.e. if you request prediction for the same data point\n",
        "        several times, the predicted class is going to be the same.\n",
        "        :param num_classes: number of classes to use for the classifier.\n",
        "        \"\"\"\n",
        "        self.prediction_dictionary = {}\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def add_data(self, data):\n",
        "        \"\"\"\n",
        "        This simulates training of a regular classifier such as logistic regression or XGBooost\n",
        "        although this classifier does not learn any model. Instead, it memorizes the data and assigns\n",
        "        to each of the data points random class taken uniformly at random from the range [0, num_classes - 1]\n",
        "        :param data: numpy array of m rows and n columns where each row is a training example.\n",
        "        \"\"\"\n",
        "        for i in range(data.shape[0]):\n",
        "            training_example = data[i]\n",
        "            self.prediction_dictionary[hash(training_example.tostring())] = np.random.randint(0, self.num_classes - 1)\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"\n",
        "        This simulates prediction. The model takes every testing data point, examines memorized predictions\n",
        "        for each of them and returns memorized random class.\n",
        "        :param test_data: numpy array of m rows and n columns where each row is a testing example.\n",
        "        :return: numpy array of size m each element of which is a predicted class for the corresponding\n",
        "        testing data point.\n",
        "        \"\"\"\n",
        "        predictions = np.zeros(test_data.shape[0])\n",
        "        for i in range(test_data.shape[0]):\n",
        "            testing_example = test_data[i]\n",
        "            if hash(testing_example.tostring()) not in self.prediction_dictionary:\n",
        "                raise ValueError('The testing example is not in self.prediction_dictionary. This random classifier requires all data points to be introdused using add_data() method.')\n",
        "            predictions[i] = self.prediction_dictionary[hash(testing_example.tostring())]\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY8TaRQEsNhD",
        "colab_type": "text"
      },
      "source": [
        "This part shows that you can get any accuracy up to 100% from models predicting labels randomly by applying inference\n",
        "the algorithm suggested by Babayan et al. at Predicting reservoir hosts and arthro-pod vectors from evolutionary\n",
        "signatures in RNA virus genomes (https://science.sciencemag.org/content/362/6414/577).\n",
        "\n",
        "Here we load data from the original paper similarly to other reproductions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HGjmpOCsa5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('BabayanEtAl_VirusData.csv', low_memory=False)\n",
        "\n",
        "experiment_data = data[data['Reservoir'].isin(['Artiodactyl',\n",
        "                                               'Carnivore',\n",
        "                                               'Fish',\n",
        "                                               'Galloanserae',\n",
        "                                               'Insect',\n",
        "                                               'Neoaves',\n",
        "                                               'Plant',\n",
        "                                               'Primate',\n",
        "                                               'Pterobat',\n",
        "                                               'Rodent',\n",
        "                                               'Vespbat'])]\n",
        "\n",
        "experiment_data = experiment_data.replace({'Reservoir': ['Artiodactyl', 'Carnivore', 'Fish', 'Galloanserae', 'Insect', 'Neoaves', 'Plant', 'Primate', 'Pterobat', 'Rodent', 'Vespbat']},\n",
        "                                          {'Reservoir': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n",
        "\n",
        "data_array = experiment_data.iloc[:, 6:].to_numpy()\n",
        "label_array = experiment_data['Reservoir'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItY6sQzlspwB",
        "colab_type": "text"
      },
      "source": [
        "Here we create `num_models` random classifiers using this data. All of the classifiers just arrange a random class to every data point and remembers it to use for consistent prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs-5trv-sqyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_models = 100\n",
        "classifiers = []\n",
        "for _ in range(num_models):\n",
        "    new_classifier = RandomClassifier(num_classes=11)\n",
        "    new_classifier.add_data(data_array)\n",
        "    classifiers.append(new_classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYyVLSpystbf",
        "colab_type": "text"
      },
      "source": [
        "Let us see the actual accuracy of these classifiers to make sure it is a random guess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOO1oLOszbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "742b23ec-34dc-4908-dc9d-db70e86e47be"
      },
      "source": [
        "accuracy = np.zeros(num_models)\n",
        "\n",
        "for model_id in range(num_models):\n",
        "    prediction = classifiers[model_id].predict(data_array)\n",
        "    accuracy[model_id] = np.sum(prediction==label_array)/prediction.shape\n",
        "\n",
        "print('Mean accuracy for {} models is: {}'.format(num_models, np.mean(accuracy)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean accuracy for 100 models is: 0.09267734553775744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zY8izM-tU0v",
        "colab_type": "text"
      },
      "source": [
        "Let us define `num_subsets` for the number of random subsets of the size `subset_size`. These subsets will be taken from the data at random. We will also add the focal point to each of them to make sure that all subsets contain it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA6BFi5UtwvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_subsets = 550\n",
        "subset_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4almIOrs7Uo",
        "colab_type": "text"
      },
      "source": [
        "Let us make a routine to predict a focal point using the definition from the original paper. It will be the point for which we make predictions. The focal point will be a data point number `focal_point_id`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaCARzJEtMmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_focal_point(focal_point_id):\n",
        "  subsets_data = []\n",
        "  subsets_labels = []\n",
        "  for _ in range(num_subsets):\n",
        "    random_indices = np.random.randint(0, data_array.shape[0], subset_size)\n",
        "    subsets_data.append(data_array[random_indices])\n",
        "    subsets_labels.append(label_array[random_indices])\n",
        "  \n",
        "  # Let us calculate accuracy of each of our random classifiers on every of the subsets.\n",
        "\n",
        "  model_performance = []\n",
        "\n",
        "  for model in classifiers:\n",
        "      model_accuracy = [model.predict(subsets_data[i]) == subsets_labels[i] for i in range(len(subsets_data))]\n",
        "      model_performance.append((np.mean(model_accuracy)))\n",
        "\n",
        "  # Here we get the indices of the models with the highest accuracy on the subset, get 25% of them and\n",
        "  # make prediction for the focal point.\n",
        "\n",
        "  indices_accent = np.argsort(model_performance)\n",
        "  top_25_classifier_indices = indices_accent[int(len(indices_accent)*0.75):]\n",
        "  top_25_classifiers = [classifiers[i] for i in top_25_classifier_indices]\n",
        "\n",
        "  top_25_predictions = [classifier.predict(np.array([data_array[focal_point_id]]))[0] for classifier in top_25_classifiers]\n",
        "\n",
        "  prediction = stats.mode(top_25_predictions)[0]\n",
        "\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h30VBoWFutcj",
        "colab_type": "text"
      },
      "source": [
        "Now, let us calculate the prediction for every point in the dataset and assess the accuracy of the suggested inference algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvyXc4Ryu1uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for focal_point_id in range(data_array.shape[0]):\n",
        "    focal_point_prediction = predict_focal_point(focal_point_id)\n",
        "\n",
        "accuracy = np.sum(np.array(predictions)==label_array)/label_array.shape\n",
        "\n",
        "print('Overall accuracy is: {}'.format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
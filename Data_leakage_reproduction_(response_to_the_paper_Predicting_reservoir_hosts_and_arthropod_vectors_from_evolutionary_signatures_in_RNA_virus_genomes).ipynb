{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data leakage reproduction (response to the paper Predicting reservoir hosts and arthropod vectors from evolutionary signatures in RNA virus genomes).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNc1uX3r0x1oK4rKQb0Vu4g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shelpuk/ViralHostPredictor/blob/master/Data_leakage_reproduction_(response_to_the_paper_Predicting_reservoir_hosts_and_arthropod_vectors_from_evolutionary_signatures_in_RNA_virus_genomes).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8fOv28NhKpR",
        "colab_type": "text"
      },
      "source": [
        "# Data leakage reproduction\n",
        "(in response to: Babayan, S. A., Orton, R. J. & Streicker, D. G. Predicting reservoir hosts and arthropod vectors from evolutionary signatures in rna virus genomes. Science 362, 577â€“580 (2018). URL https://science.sciencemag.org/content/362/6414/577.)\n",
        "\n",
        "**Olha Romaniuk, Sergii Shelpuk**\n",
        "\n",
        "***DeepTrait***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI1-jxkVq0YC",
        "colab_type": "text"
      },
      "source": [
        "This code illustrates infinitely powerful [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/) introduced by the inference mechanism suggested in *Study-wide accuracies* chapter of the Supplementary Materials for the [original paper](https://science.sciencemag.org/content/362/6414/577).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l54yEAGQqqPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFdSw34rciP",
        "colab_type": "text"
      },
      "source": [
        "Class `RandomClassifier` implements random classifier. For every data point, it assigns a random class label. The classifier assigns this label consistently. I.e. if you request prediction for the same data point several times, the predicted class is going to be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6TTfmmfrZH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomClassifier(object):\n",
        "    def __init__(self, num_classes):\n",
        "        \"\"\"\n",
        "        This class implements random classifier. For every data point, it assigns a random class label.\n",
        "        The classifier assigns this label consistently. I.e. if you request prediction for the same data point\n",
        "        several times, the predicted class is going to be the same.\n",
        "        :param num_classes: number of classes to use for the classifier.\n",
        "        \"\"\"\n",
        "        self.prediction_dictionary = {}\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def add_data(self, data):\n",
        "        \"\"\"\n",
        "        This simulates training of a regular classifier such as logistic regression or XGBooost\n",
        "        although this classifier does not learn any model. Instead, it memorizes the data and assigns\n",
        "        to each of the data points random class taken uniformly at random from the range [0, num_classes - 1]\n",
        "        :param data: numpy array of m rows and n columns where each row is a training example.\n",
        "        \"\"\"\n",
        "        for i in range(data.shape[0]):\n",
        "            training_example = data[i]\n",
        "            self.prediction_dictionary[hash(training_example.tostring())] = np.random.randint(0, self.num_classes - 1)\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"\n",
        "        This simulates prediction. The model takes every testing data point, examines memorized predictions\n",
        "        for each of them and returns memorized random class.\n",
        "        :param test_data: numpy array of m rows and n columns where each row is a testing example.\n",
        "        :return: numpy array of size m each element of which is a predicted class for the corresponding\n",
        "        testing data point.\n",
        "        \"\"\"\n",
        "        predictions = np.zeros(test_data.shape[0])\n",
        "        for i in range(test_data.shape[0]):\n",
        "            testing_example = test_data[i]\n",
        "            if hash(testing_example.tostring()) not in self.prediction_dictionary:\n",
        "                raise ValueError('The testing example is not in self.prediction_dictionary. This random classifier requires all data points to be introdused using add_data() method.')\n",
        "            predictions[i] = self.prediction_dictionary[hash(testing_example.tostring())]\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY8TaRQEsNhD",
        "colab_type": "text"
      },
      "source": [
        "This part shows that you can get any accuracy up to 100% from models predicting labels randomly by applying inference\n",
        "the algorithm suggested by Babayan et al. at Predicting reservoir hosts and arthro-pod vectors from evolutionary\n",
        "signatures in RNA virus genomes (https://science.sciencemag.org/content/362/6414/577).\n",
        "\n",
        "Here we load data from the original paper similarly to other reproductions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HGjmpOCsa5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('BabayanEtAl_VirusData.csv', low_memory=False)\n",
        "\n",
        "experiment_data = data[data['Reservoir'].isin(['Artiodactyl',\n",
        "                                               'Carnivore',\n",
        "                                               'Fish',\n",
        "                                               'Galloanserae',\n",
        "                                               'Insect',\n",
        "                                               'Neoaves',\n",
        "                                               'Plant',\n",
        "                                               'Primate',\n",
        "                                               'Pterobat',\n",
        "                                               'Rodent',\n",
        "                                               'Vespbat'])]\n",
        "\n",
        "experiment_data = experiment_data.replace({'Reservoir': ['Artiodactyl', 'Carnivore', 'Fish', 'Galloanserae', 'Insect', 'Neoaves', 'Plant', 'Primate', 'Pterobat', 'Rodent', 'Vespbat']},\n",
        "                                          {'Reservoir': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n",
        "\n",
        "data_array = experiment_data.iloc[:, 6:].to_numpy()\n",
        "label_array = experiment_data['Reservoir'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItY6sQzlspwB",
        "colab_type": "text"
      },
      "source": [
        "Here we create `num_models` random classifiers using this data. All of the classifiers just arrange a random class to every data point and remembers it to use for consistent prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs-5trv-sqyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_models = 100\n",
        "classifiers = []\n",
        "for _ in range(num_models):\n",
        "    new_classifier = RandomClassifier(num_classes=11)\n",
        "    new_classifier.add_data(data_array)\n",
        "    classifiers.append(new_classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYyVLSpystbf",
        "colab_type": "text"
      },
      "source": [
        "Let us see the actual accuracy of these classifiers to make sure it is a random guess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjOO1oLOszbJ",
        "colab_type": "code",
        "outputId": "30179f66-a006-4f60-bbd7-034936995f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "accuracy = np.zeros(num_models)\n",
        "\n",
        "for model_id in range(num_models):\n",
        "    prediction = classifiers[model_id].predict(data_array)\n",
        "    accuracy[model_id] = (np.sum(prediction==label_array))/prediction.shape\n",
        "\n",
        "print('Mean accuracy for {} models is: {}'.format(num_models, np.mean(accuracy)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean accuracy for 100 models is: 0.09377574370709382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zY8izM-tU0v",
        "colab_type": "text"
      },
      "source": [
        "Let us define the number of random points in every cross-validation (CV) set as `subset_size`. These sets will correspond to k-fold cross-validation sets so that each model has exactly one corresponding CV set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA6BFi5UtwvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subset_size = 50\n",
        "\n",
        "subsets_data = []\n",
        "subsets_labels = []\n",
        "subsets_indices = []\n",
        "for _ in range(num_models):\n",
        "  random_indices = np.random.randint(0, data_array.shape[0], subset_size)\n",
        "  subsets_data.append(data_array[random_indices])\n",
        "  subsets_labels.append(label_array[random_indices])\n",
        "  subsets_indices.append(random_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4almIOrs7Uo",
        "colab_type": "text"
      },
      "source": [
        "Let us make a routine to predict a focal point using the definition from the original paper. The focal point will be a data point number `focal_point_id`. We will also include it to every CV set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaCARzJEtMmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_focal_point(focal_point_id):\n",
        "\n",
        "  # Let us calculate accuracy of each of our random classifiers on every of the subsets.\n",
        "\n",
        "  model_performance = []\n",
        "\n",
        "  for model_id in range(len(classifiers)):\n",
        "    # Checking if focal point is in the CV set for the model number `model_id`.\n",
        "    # If it is, we will use corresponding CV set as is. If it is not, we will\n",
        "    # add focal point to the CV set.\n",
        "    if focal_point_id in subsets_indices[model_id]:\n",
        "      test_data = subsets_data[model_id]\n",
        "      test_labels = subsets_labels[model_id]\n",
        "    else:\n",
        "      test_data = np.append(subsets_data[model_id], [data_array[focal_point_id]], axis=0)\n",
        "      test_labels = np.append(subsets_labels[model_id], [label_array[focal_point_id]], axis=0)\n",
        "\n",
        "    model_accuracy = np.sum(classifiers[model_id].predict(test_data) == test_labels) / test_labels.shape[0]\n",
        "\n",
        "    model_performance.append(model_accuracy)\n",
        "\n",
        "  # Here we get the indices of the models with the highest accuracy on CV sets, get 25% of them and\n",
        "  # make prediction for the focal point.\n",
        "\n",
        "  indices_accent = np.argsort(model_performance)\n",
        "  top_25_classifier_indices = indices_accent[int(len(indices_accent)*0.75):]\n",
        "  top_25_classifiers = [classifiers[i] for i in top_25_classifier_indices]\n",
        "\n",
        "  top_25_predictions = [classifier.predict(np.array([data_array[focal_point_id]]))[0] for classifier in top_25_classifiers]\n",
        "\n",
        "  # Here we implement bagging mechanism for the best 25% of the models.\n",
        "  \n",
        "  prediction = stats.mode(top_25_predictions)[0]\n",
        "\n",
        "  return int(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h30VBoWFutcj",
        "colab_type": "text"
      },
      "source": [
        "Now, let us calculate the prediction for every point in the dataset and assess the accuracy of the suggested inference algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvyXc4Ryu1uT",
        "colab_type": "code",
        "outputId": "3711e2f5-d063-42b4-b170-805dd4c4ba84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for focal_point_id in range(data_array.shape[0]):\n",
        "    focal_point_prediction = predict_focal_point(focal_point_id)\n",
        "    predictions.append(focal_point_prediction)\n",
        "\n",
        "final_accuracy = np.sum(np.array(predictions)==label_array)/label_array.shape[0]\n",
        "\n",
        "print('Overall accuracy is: {}'.format(final_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall accuracy is: 0.38443935926773454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5h1SGejd0bQ",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the data leakage introduced by the suggested inference mechanism can get 38% accuracy out of the model that predicts labels completely arbitrarily (at random).\n",
        "\n",
        "Below we show that increasing the number of models and decreasing the CV set size can bring this accuracy up to 90%+. The models themselves still predict hosts arbitrarily, the accuracy is explained entirely by the data leakage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp_JDVCyeyPf",
        "colab_type": "code",
        "outputId": "3966fc47-e279-4810-d4b9-dfd1e569427a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_models = 1000\n",
        "classifiers = []\n",
        "for _ in range(num_models):\n",
        "    new_classifier = RandomClassifier(num_classes=11)\n",
        "    new_classifier.add_data(data_array)\n",
        "    classifiers.append(new_classifier)\n",
        "\n",
        "subset_size = 5\n",
        "\n",
        "subsets_data = []\n",
        "subsets_labels = []\n",
        "subsets_indices = []\n",
        "for _ in range(num_models):\n",
        "  random_indices = np.random.randint(0, data_array.shape[0], subset_size)\n",
        "  subsets_data.append(data_array[random_indices])\n",
        "  subsets_labels.append(label_array[random_indices])\n",
        "  subsets_indices.append(random_indices)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for focal_point_id in range(data_array.shape[0]):\n",
        "    focal_point_prediction = predict_focal_point(focal_point_id)\n",
        "    predictions.append(focal_point_prediction)\n",
        "\n",
        "final_accuracy = np.sum(np.array(predictions)==label_array)/label_array.shape\n",
        "\n",
        "print('Overall accuracy is: {}'.format(final_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall accuracy is: [0.92677346]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}